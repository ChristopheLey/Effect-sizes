---
title             : "What measure of effect size when comparing two groups based on their means?"

shorttitle        : "Effect size"

author: 
  - name          : "Marie Delacre" 
    affiliation   : "1"
    corresponding : yes    
    address       : "CP191, avenue F.D. Roosevelt 50, 1050 Bruxelles"
    email         : "marie.delacre@ulb.ac.be"

  - name          : "Christophe Leys"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Université Libre de Bruxelles, Service of Analysis of the Data (SAD), Bruxelles, Belgium"
    
authornote: |

abstract: |

keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output:
  papaja::apa6_pdf:
    includes:
      fig_caption       : FALSE
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Intro

During decades, researchers in social science [@Henson_Smith_2000] and education [@Fan_2001] have overestimated the ability of the null hypothesis (H0) testing to determine the importance of their results. The standard for researchers in social science is to define H0 as the absence of effect [@Meehl_1990]. For example, when comparing the mean of two groups, researchers commonly test the H0 that there is no mean differences between groups [@Steyn_2000]. Any effect that is significantly different from zero will be seen as sole support for a theory. 

Such an approach has faced many criticisms among which the most relevant to our concern is that the null hypothesis testing highly depends on sample size: for a given alpha level and a given difference between groups, the larger the sample size, the higher the probability of rejecting the null hypothesis [@Fan_2001; @Sullivan_Feinn_2012; @Olejnik_Algina_2000; @Kirk_2009]. It implies that even tiny differences could be detected as statistically significant with very large sample sizes [@McBride_et_al_1993]\footnote{Tiny differences might be due to sampling error, or to other factors than the one of interest: even under the assumption of random assignent (which is a necessary but not sufficient condition), it is almost impossible to be sure that the only difference between two conditions is the one defined by the factor of interest. Other tiny factors of no theoretical interest might slighly influence results, making the probability of getting an actual zero effect very low. This is what Meehl (1990) calls 'systematic noise'}. 

Facing this argument, it has become an adviced practice to report the *p*-value assorted by a measure of the effect size, that is, a quantitative measure of the magnitude of the experimental effect [@Fan_2001; @Hays_1963;@Cohen_1965]. This practice is also highly endorsed by the American Psychological Association (APA) and the American Educational Research Association (AERA) [@APA_2010;@AERA_2006]. However, limited studies properly report effect size in the last several decades. 

First, there is a high confusion between the effect size and other related concepts such as the [TROUVER UN TERME] significance (e.g. clinical, personnal, social, professionnal) of a result (i.e. the relevance of an effect in real life). Moreover, there are several situations that call for effect size measures and in the current litterature, it's not always easy to know which measure using in a specific context. The first aim of this paper is therefore twofold:   
1. Clearly define what is (and what is not) a measure of effect size;  
2. Listing the different situations that call for effect sizes measure and define required properties as a function of the situations;

Second, many differents estimators of effect sizes are available in literature and it is not always easy to know which measures is appropriate in which circumstance. We will limit our study to “between-subject” designs where individuals are randomly assigned into one of two independant groups and groups scores are compared based on their means. More specifically, we will focus on the standardized mean difference, called the *d*-family, because it is the dominant family of estimators of effect size when comparing two groups based on their means [@Peng_et_al_2013; @Shieh_2013]. We will see that even in this very specific context, there is little agreement between researchers as to which is the most suitable estimator. According to us, the main reason is that it is difficult to find a measure which optimally serves all the purposes of an effect size measure. For example, interpretability is obtained at the expense of inferential properties and vice versa. The second aim of this paper is therefore to review the most famous estimators of this family next to the role they serve.

# Measure of effect size: what it is, what it is not 

The effect size is commonly refered to the practical significance of a test. @Grissom_and_kim_2005 define the effect size as the extent to which results differ from what is implied by the null hypothesis. In the context of the comparison of two groups based on their mean, depending on the defined null hypothesis (considering the absence of effect as the null hypothesis), we could define the effect size either as the magnitude of differences between parameters of two populations groups are extracted from [e.g. the mean; @Peng_and_Chen_2014] or as the magnitude of the relation between one dichotomous factor and one dependent variable [@AERA_2006]. Both definitions refers to as the most famous families of measures of effect sizes [Rosenthal_1994]: respectively the *d*-family and the *r*-family.

Very often, the contribution of the measures of effect size is overestimated. 

First, benchmarks about what should be a small, medium or large effect size might have contribued at seeing the effect size as a measure of the importance or the relevance of an effect in real life, but it is not [@Stout_Ruble_1995]. The effect size is only a mathematical indicator of the magnitude of a difference, which depends on the way a variable is converted into numerical indicator. In order to assess the meaningfulness of an effect, we should be able to relate this effect with behaviors/meaningful consequences in the real world [@Andersen_et_al_2007]. For example, let us imagine a sample of students in serious school failure who are randomly divided into two groups: an experimental group following a training program and a control group. At the end of the training, students in the experimental group have on average significantly higher scores on a test than students in the control group, and the difference is large (e.g. 30 percents). Does it mean that students in the experimental condition will be able to pass to the next grade and to continue normal schooling? Whether the computed magnitude of difference is an important, meaningful change in everyday life refers to another construct: the *??? significance* [@Bothe_Richardson_2011]. It refers to the interpretation of treatment outcomes and is neither statistical nor mathematical, it is related to underlying theory that posits an empirical hypothesis. In other words, the relation between *practical* and *???* significance is more a theoretical argument than a statistical one. 
 
Second, in the context of the comparison of two groups based on their means, it should not replace the null hypothesis testing. Statistical testing allows the researcher to determine whether the oberved departure from H0 occured by chance or not [@Stout_Ruble_1995] while effect size estimators allow to assess the practical signficance of an effect, and as reminds @Fan_2001 "a practically meaningful outcome may also have occured by chance, and consequently, is not trustworthy". For this reason, the use of confidence intervals around the effect size estimate is highly recommended [@Bothe_Richardson_2011].

# Different goals of measures of effect sizes

Effect size measures can be used for *inferential* purposes:   
- The effect sizes from previous studies can be used in a priori power analysis when planning a new study [@Sullivan_Feinn_2012;@Lakens_2013;@Stout_Ruble_1995;@Prentice_Miller_1992; @Wilkinson_1999]  
- To compute confidence intervals [@Shieh_2013]

When used for inference, effect size measures are generally submitted to a range of assumptions (e.g. independent and identically distributed residuals are normal and have equal variances between groups). When these assumptions are not met, many estimations of effect size are inaccurate and alter the robustness of the statistical conclusions. 

Measures of effect size can also be used for *comparative* purposes, i.e. to assess the stability of results across designs, analysis, samples sizes [@Wilkinson_1999]. It includes:    
- To compare results of 2 or more studies [@Prentice_Miller_1992]  
- To incorporate results in meta-analysis [@Lakens_2013;@Wilkinson_1999;@Stout_Ruble_1995; @Nakagawa_and_Cuthill_2007;@Li_2016]  

Finally, effect size measures can be used for *interpretive* purposes: in order to assess the practical significance of a result [beyond statistical significance; @Lakens_2013;@Prentice_Miller_1992;@APA_2010]  

# Robust measures

## Properties of a good effect size estimator (for inferential purposes)

The value of the estimate of an estimator depends on the sampling. That is to say, based on different samples extracted from the same population, one would  obtain different estimates of the same estimator. The *sampling distribution* of the estimator is the distribution of all estimates, based on all possible samples of size *n* extracted from one population. Studying the sampling distribution is very useful, as it allows to assess the goodness of an effect size estimator and more specifically, three desirable properties of a good estimator: **unbiasedness**, **consistency** and **efficiency**.

An estimator is unbiased if the distribution of estimates is centered around the true population parameter. On the other hand, an estimator is positively (or negatively) biased if the distribution is centered around a value that is higher (or smaller) than the true populatione parameter (see Figure \ref{fig:BIAS}). In other words, the bias tells us if estimates are good, on average. The *bias* of a point estimator $\hat{\delta}$ can be computed as follows:

\begin{equation} 
\hat{\delta}_{bias}=E(\hat{\delta})-\delta
(\#eq:BIAS)
\end{equation} 

Where $E(\hat{\delta})$ is the mean of the sampling distribution of the estimator and $\delta$ is the true parameter. 

In order to compare the *bias* of a point estimator for different true population parameters, we can compute the bias divided by $\delta$.

\begin{equation} 
\hat{\delta}_{bias}=\frac{E(\hat{\delta})-\delta}{\delta}
(\#eq:BIASRATE)
\end{equation} 

```{r "BIAS", echo=FALSE, fig.width = 15,fig.height=8,out.width = '400px',fig.cap = "Samplig distribution for a positively biased (left), an unbiased (center) and a negatively biased estimator (right)"}
par(mai=c(.5,1,.5,1),mfrow=c(1,3),cex.axis=2)
plot(function(x) dnorm(x,mean=2,sd=2),-6, 10,xlab="",ylab="",main ="",axes=F,lty=1,xlim=c(-6,10),ylim=c(0,.2),lwd=1,cex.lab=1.2)
segments(2,0,2,dnorm(2,mean=2,sd=2))
segments(0,0,0,dnorm(0,mean=2,sd=2),lty=2,lwd=2)
axis(side=1,at=seq(-6,10,2),labels=c(rep("",3),expression(delta),expression(paste("E(",hat(delta),")")),rep("",4)),col="white",cex=2)

plot(function(x) dnorm(x,mean=0,sd=2),-8, 8,xlab="",ylab="",main ="",axes=F,lty=1,xlim=c(-8,8),ylim=c(0,.2),lwd=1,cex.lab=1.2)
segments(0,0,0,dnorm(0,mean=0,sd=2))
segments(0,0,0,dnorm(0,mean=0,sd=2),lty=2,lwd=2)
axis(side=1,at=seq(-6,10,2),col="white",labels=c(rep("",3),expression(paste(delta,"=E(",hat(delta),")")),rep("",5)))

plot(function(x) dnorm(x,mean=-2,sd=2),-10, 6,xlab="",ylab="",main ="",axes=F,lty=1,xlim=c(-10,6),ylim=c(0,.2),lwd=1,cex.lab=1.2)
segments(-2,0,-2,dnorm(2,mean=2,sd=2))
segments(0,0,0,dnorm(0,mean=2,sd=2),lty=2,lwd=2)
axis(side=1,at=seq(-6,10,2),labels=c(rep("",2),expression(paste("E(",hat(delta),")")),expression(delta),rep("",5)),col="white")

```

Bias informs us about the goodness of estimates averages, but says nothing about individual estimates. Imagine a situation where the distribution of estimates is centered around the real parameter but with such a large variance that some point estimates are very far from the center. It would be problematic, as long as we have only one estimate, the one based on our sample, and we don't know how far is this estimate from the center of the sampling distribution. We hope that *all* possible estimates are close enough of the true population parameter, in order to be sure that for *any* estimate, one has a correct estimation of the real parameter. In other words, we expect the variability of estimates around the true population parameter to be as small as possible. It refers to the **efficiency** of the point estimator ($\hat{\delta}$) and can be computed as follows:

\begin{equation} 
\hat{\delta}_{efficiency}=Var(\hat{\delta})
(\#eq:EFFICIENCY)
\end{equation} 

Among all unbiased estimators, the more efficient will be the one with the smallest variance. 

Note that both unbiasedness and efficiency are very important. Remember that we hope that *any* possible estimate is close of the real parameter. An unbiased estimator with such a large variance that somes estimates are extremely far from the real parameter is as undesirable as a parameter which is highly biased. In some situations, it is better to have a very slightly biased estimator with a tigh shape around the biased value, so each estimate "misses" the real parameter a little, than a biased estimator with a large variance [Ref to add: https://eranraviv.com/bias-vs-consistency/]. Because both *unbiasedness* and *efficiency* must be considered, it is interesting to compute an indicator that take simultaneously both properties into account [@Wackerly_et_al_2008]. The *mean square error* of a point estimator $\hat{\delta}$ is defined as follows:

\begin{equation} 
MSE(\hat{\delta})=E[(\hat{\delta}-\delta)^2]
(\#eq:MSE1)
\end{equation} 

It can be proven that the *mean square error* is a function of the bias and the variance of $\hat{\delta}$ :

\begin{equation} 
MSE(\hat{\delta})=\hat{\delta}_{efficiency}+\hat{\delta}_{bias}^2
(\#eq:MSE2)
\end{equation} 

Finally, the last property of a good point estimator is **consistency**: consistency means that the bigger the sample size, the closer the estimate of the population parameter. In other words, the estimates *converge* to the true population parameter. 

## Properties of a good effect size estimator (for comparative and interpretive purposes)

**Generality**

According to [@Cumming_2013], an effect size estimator need to have a constant value across designs in order to be easily interpretable and to be included in meta-analysis. 

At first glance, this quality is incompatible with the mathematical properties required for an inferential purpose. For example, according to the statistical properties of Welch’s statistic, in the context of heteroscedasticity, it seems required to take the sample size allocation ratio into account in order to define a proper inferential measure of effect size [@Shieh_2103]. It would mean that for the same amount of differences between two means, same standard deviations and $\sigma$-ratio, a proper effect size estimator would vary as a function of the sample sizes allocation ratio, which would make it very dependent on the characteristics of the design and therefore very difficult to interpret.

In the following section, we will review the most popular effect size measures in the *d* family. In this section, we will also propose an original transformation of the Shieh's $\delta$. This transformation should help at bridging the imperatives of generality and need to take the specificities of the design into account.

# Different measures of effect sizes

As previously mentioned, we will limit our study to the *d*-family, commonly used with “between-subject” designs where individuals are randomly assigned into one of two independant groups and groups scores are compared based on their means. We will first focus our attention on the inferential purpose of the effect size measures.

The population effect size is defined as follows: 
  
\begin{equation} 
\delta = \frac{\mu_{1}-\mu_{2}}{\delta} 
(\#eq:Cohendelta)
\end{equation} 
  
They exist different estimators of this effect size measure varying as a function of the chosen standardizer ($\delta$). For all estimators, the mean difference is estimated by the difference of both sample means ($\bar{X}_1-\bar{X}_2$). When used for inference, some of them rely on both assumptions of normality and equality of variances, while others rely solely on the normality assumption.

## Alternatives when variances are equal between groups

The most common estimator of $\delta$ is Cohen's $d_{s}$ where the sample mean difference is divided by a pooled error term [@Cohen_1965]: 

\begin{equation} 
Cohen's d_s = \frac{\bar{X}_1-\bar{X}_2}{\sqrt{\frac{(n_1-1) \times SD_1+(n_2-1) \times SD_2}{n_1+n_2-2}}} 
(\#eq:Cohends)
\end{equation} 

The reasoning behind this measure is that considering both sample as extracted from a common population variance [@Keselman_et_al_2008], we achieve a more accurate estimation of the population variance by pooling both estimates of this parameter (i.e $SD_1$ and $SD_2$) and because the larger the sample size, the more accurate the estimate, we give more weight to the estimate based on the larger sample size ($max(n_j)$). Unfortunately, even under the assumption of normality and equality of variances, Cohen's $d_s$ is known to be positively biased [Thompson, 2006, cited by Lakens, 2013] and for this reason, @Hedges_Olkin_1985 has defined a bias-corrected version,  which is referred to: 

\begin{equation} 
Hedge's \space g_s = Cohen's \space d_s \times (1-\frac{3}{4 \times (n_1+n_2)-9}) 
(\#eq:Hedgesgs)
\end{equation} 

The pooled SD is the best choice when variances are equal between groups  [@Grissom_Kim_2001] but they may not be well advised for use with data that violates this assumption [@Kelley_2005;@Grissom_Kim_2001; @Cumming_2013; @Grissom_and_kim_2005; @Kelley_2005; @Shieh_2013]. In case of a positive pairing (i.e. the group with the larger sample size also has the larger variance), the variance will be over-estimated and therefore, the estimator will be lower as it should be. On the other side, in case of negative pairing (i.e. the group with the larger sample size has the smaller variance), the estimator will be larger as it should be. However, this assumption is very rare in practice [@Glass_et_al_1972; @Cain_et_al_2017; @Micceri_1989; @Yuan_et_al_2004; @Erceg-Hurn_Mirosevich_2008;@Grissom_2000; @Delacre_et_al_2017; @Delacre_et_al_2019]. 

## Alternatives when variances are unequal between groups

While it is becoming more common in statistical software to present Welch’s *t*-test by default, when performing a *t*-test (i.e., R, Minitab), similar issues for the measures of effect sizes has received less attention [@Shieh_2013] and Cohen’s $d_s$ remains persistent \footnote{For example, in Jamovi, Cohen's ds is provided, whatever one performs Student's or Welch's t-test}. One possible reason is that researchers cannot find a consensus on which alternative should be in use [@Shieh_2013]. In his review, @Shieh_2013 mention three alternative available in the literature: the sample mean difference, divided by the non pooled average of both variance estimates, the Shieh's $d_s$ (B) and the Glass's $d_s$ (C).  

The **sample mean difference, divided by the non pooled average of both variance estimates** (A) was suggested by Cohen (1988). We immediately exclude this alternative because it suffers of many limitations:  
- it results in a variance term of an artificial population and is therefore very difficult to interpret [@Grissom_Kim_2001]  
- unless both sample sizes are equal, the variance term does not correspond to the variance of the mean difference [@Shieh_2013]  
- unless the mean difference is null, the measure is biased. Moreover, the bigger the sample size, the larger the variance around the estimate (Note: Huynh, 1989, propose un estimateur ajusté).   
Formule trouvée sur un forum : https://stats.stackexchange.com/questions/210352/do-cohens-d-and-hedges-g-apply-to-the-welch-t-test

### Shieh's d
    
Kulinskaya and Staudte (2007) adviced the use of a standardizer that take the sample sizes allocation ratios into account, in addition to the variance of both groups. It results in a modification of the exact *SD* of the sample mean difference: 
      
\begin{equation} 
Shieh's \space d_s = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{SD_1^2/q_1+SD_2^2/q_2}}
(\#eq:Shiehds)
\end{equation} 

According to the statistical properties of Welch's statistic, in the context of heteroscedasticity, it seems required to take the sample size allocation ratio into account in order to define a proper inferential measure of effect size [@Shieh_2013]. At the same time, the lack of generality caused by taking this specificity of the design into account has led @Cumming_2013 to question its usefulness in terms of interpretability: when keeping constant the mean difference as well as $SD_1$ and $SD_2$,  Shieh's $d_s$ will vary as a function of the sample sizes allocation ratio (dependency of Shieh's $d_s$ value on the sample sizes allocation ratio is detailed and illustrated in Appendix 1, and also in the following shiny application: http://127.0.0.1:4350/). Fortunately, it can be shown that there is a relationship between the Shieh's $d_s$ value when samples sizes are equal between groups (i.e. $\delta_{Shieh, n1=n2}$) and Shieh's $d_s$ values for all other sample sizes allocation ratios

\begin{equation} 
\delta_{Shieh,n_1=n_2}= \delta_{Shieh} \times \frac{\frac{\mu_1-\mu_2}{2 \times \sigma_{(n_1=n_2)}}}{\frac{(\mu_1-\mu_2) \times \sqrt{nratio}}{(nratio+1) \times \sigma_{(n_1\neq n_2)}}}
\leftrightarrow \delta_{Shieh,n_1=n_2}= \delta_{Shieh} \times \frac{(nratio+1) \times \sigma_{n_1 \neq n_2}}{2 \times \sigma_{n_1=n_2} \times \sqrt{nratio}}
(\#eq:shiehvsbaldesign)
\end{equation} 

With $$\sigma_{n_1=n_2}= \sqrt{\frac{\sigma_1^2+\sigma_2^2}{2}}$$ and 
$$\sigma_{n_1 \neq n_2} = \sqrt{(1- \frac{n_1}{N}) \times \sigma_1^2+(1- \frac{n_2}{N}) \times \sigma_2^2}$$

Results in equation \ref{eq:shiehvsbaldesign} could be interpreted as "which Shieh's d value would we obtain if design of our study was balanced (i.e. $n_{1} = n_{2}$)?". Thanks to this equation, Shieh's $\delta$ can have a practical significance, independently of the sample sizes allocation ratio.  
    

### Glass's $d_s$
      
When comparing one control group with one experimental group, Glass, Smith, & McGaw (1981) recommend using the SD of the control group as standardizer. It is also advocated by [@Cumming_2013], because according to him, it is what makes the most sense, conceptually speaking. 
    
\begin{equation} 
Glass's \space d_s = \frac{\bar{X}_{experimental} - \bar{X}_{control}}{SD_{control}}
(\#eq:Glassds)
\end{equation} 

Because the SD of the experimental group has no impact on the computed Glass's $d_s$, one could advice to report both mean differences divided by the SD of the control group and mean differences divided by the SD of the experimental groups. However, it could induces large ambiguity because both measures could be substantially different [@Shieh_2013]. 
    
## Simulations

We performed Monte Carlo simulations using R (version 3.5.0) to assess the bias, efficiency and consistency of the 6 measures of effect sizes described in previous section: Cohen's $d_s$, Hedge's $g_s$, Glass's $d_s$ using respectively the sample standard deviation of the first or second group as a standardizer, Shieh's $d_s$ and our transformed measure of Shieh's $d_s$. 100 000 datasets were generated for 1260 scenarios (in 315 scenarios, samples are extracted from a normal population distribution). 

Population parameter values were chosen in order to illustrate the consequences of factors known to play a key role on goodness of estimators. We manipulated the sample sizes, the sample size ratio (*n*-ratio = $\frac{n_k}{n_j}$), the *SD*-ratio (*SD*-ratio = $\frac{\sigma_1}{\sigma_2}$), and the sample size and variance pairing. 

In our scenarios, the mean of the second sample was always 0 and the mean of the first sample varied from 0 to 4, in step of 1 (so does the mean difference between groups). Moreover, the standard deviation of the first group is always 1, and the standard deviation of the second group is .1, .25, .5, 1, 2, 4 or 10 (so does the *SD*-ratio). The simulations for which the *SD* of both samples equals 1 are the particular case of homoscedasticity (i.e. equal variances across groups). Sample size of both groups were 20, 50 or 100. When sample sizes of both groups are equal, the *n*-ratio equals 1 (it is known as a balanced design). All possible combinations of *n*-ratio and *SD*-ratio were performed in order to distinguish positive pairings (the group with the largest sample size is extracted from the population with the largest *SD*), negative pairings (the group with the smallest sample size is extracted from the population with the  smallest *SD*), and no pairing (sample sizes and/or population *SD* are equal across all groups). All these conditions were tested with normal and non-normal distributions. We used the article of @Cain_et_al_2017 in order to determine realistic population parameters values (i.e. skewness and kurtosis) for non normal distributions: ##RAJOUTER LE DETAIL ET VOIR ANNEXE POU RLA TRANSFO DE G1 ET G2##

In sum, the simulations grouped over different sample sizes yield 5 conditions based on the *n*-ratio, *SD*-ratio, and sample size and variance pairing, as summarized in Table 1. 


Table 1.
*5 conditions based on the n-ratio, SD-ratio, and sample size and variance pairing*

|                               |             |             |__*n*-ratio__|             |
| :---------------------------: | :---------: |:----------: |:-----------:|:-----------:|
|                               |             |    **1**    |    **>1**   |    **<1**   |
|                               |             |------------ |-------------|-------------|
|                               |    **1**    |      a      |      b      |      b      | 
|                               |             |             |             |             |
|      __*SD*-ratio__           |   **>1**    |      c      |      d      |      e      |
|                               |             |             |             |             |
|                               |   **<1**    |      c      |      e      |      d      |

*Note.* The *n*-ratio is the sample size of the last group divided by the sample size of the first group. When all sample sizes are equal across groups, the *n*-ratio equals 1. When the sample size of the last group is higher than the sample size of the first group, *n*-ratio > 1, and when the sample size of the last group is smaller than the sample size of the first group, *n*-ratio < 1. *SD*-ratio is the population *SD* of the first group divided by the population *SD* of the second group. When all samples are extracted from populations with the same *SD*, the *SD*-ratio equals 1. When the last group is extracted from a population with a larger *SD* than all other groups, the *SD*-ratio > 1. When the last group is extracted from a population with a smaller *SD* than all other groups, the *SD*-ratio < 1. 
